{
    "text": "WEBVTT\n\n00:00:13.005 --> 00:00:15.513\nThere was a day, about 10 years ago,\n\n00:00:15.537 --> 00:00:19.481\nwhen I asked a friend to hold\na baby dinosaur robot upside down.\n\n00:00:21.847 --> 00:00:25.293\nIt was this toy called a Pleo\nthat I had ordered,\n\n00:00:25.317 --> 00:00:29.718\nand I was really excited about it\nbecause I've always loved robots.\n\n00:00:29.742 --> 00:00:32.021\nAnd this one has really cool\ntechnical features.\n\n00:00:32.045 --> 00:00:34.164\nIt had motors and touch sensors\n\n00:00:34.188 --> 00:00:36.432\nand it had an infrared camera.\n\n00:00:36.456 --> 00:00:39.219\nAnd one of the things it had\nwas a tilt sensor,\n\n00:00:39.243 --> 00:00:41.561\nso it knew what direction it was facing.\n\n00:00:42.053 --> 00:00:44.187\nAnd when you held it upside down,\n\n00:00:44.211 --> 00:00:45.783\nit would start to cry.\n\n00:00:46.485 --> 00:00:49.981\nAnd I thought this was super cool,\nso I was showing it off to my friend,\n\n00:00:50.005 --> 00:00:52.810\nand I said, \"Oh, hold it up by the tail.\nSee what it does.\"\n\n00:00:55.226 --> 00:00:58.851\nSo we're watching\nthe theatrics of this robot\n\n00:00:58.875 --> 00:01:01.074\nstruggle and cry out.\n\n00:01:02.725 --> 00:01:04.772\nAnd after a few seconds,\n\n00:01:04.796 --> 00:01:06.768\nit starts to bother me a little,\n\n00:01:07.702 --> 00:01:11.126\nand I said, \"OK, that's enough now.\n\n00:01:11.888 --> 00:01:14.193\nLet's put him back down.\"\n\n00:01:14.217 --> 00:01:16.772\nAnd then I pet the robot\nto make it stop crying.\n\n00:01:18.931 --> 00:01:21.383\nAnd that was kind of\na weird experience for me.\n\n00:01:22.042 --> 00:01:26.611\nFor one thing, I wasn't the most\nmaternal person at the time.\n\n00:01:26.635 --> 00:01:29.366\nAlthough since then I've become\na mother, nine months ago,\n\n00:01:29.390 --> 00:01:32.823\nand I've learned that babies also squirm\nwhen you hold them upside down.\n\n00:01:32.847 --> 00:01:34.410\n(Laughter)\n\n00:01:34.981 --> 00:01:37.339\nBut my response to this robot\nwas also interesting\n\n00:01:37.363 --> 00:01:41.464\nbecause I knew exactly\nhow this machine worked,\n\n00:01:41.488 --> 00:01:44.750\nand yet I still felt\ncompelled to be kind to it.\n\n00:01:46.408 --> 00:01:49.115\nAnd that observation sparked a curiosity\n\n00:01:49.139 --> 00:01:51.971\nthat I've spent the past decade pursuing.\n\n00:01:52.869 --> 00:01:54.662\nWhy did I comfort this robot?\n\n00:01:56.186 --> 00:01:59.765\nAnd one of the things I discovered\nwas that my treatment of this machine\n\n00:01:59.789 --> 00:02:03.490\nwas more than just an awkward moment\nin my living room,\n\n00:02:03.514 --> 00:02:08.934\nthat in a world where we're increasingly\nintegrating robots into our lives,\n\n00:02:08.958 --> 00:02:12.084\nan instinct like that\nmight actually have consequences,\n\n00:02:13.410 --> 00:02:17.159\nbecause the first thing that I discovered\nis that it's not just me.\n\n00:02:19.207 --> 00:02:24.009\nIn 2007, the Washington Post\nreported that the United States military\n\n00:02:24.033 --> 00:02:27.263\nwas testing this robot\nthat defused land mines.\n\n00:02:27.287 --> 00:02:30.199\nAnd the way it worked\nwas it was shaped like a stick insect\n\n00:02:30.223 --> 00:02:32.874\nand it would walk\naround a minefield on its legs,\n\n00:02:32.898 --> 00:02:36.104\nand every time it stepped on a mine,\none of the legs would blow up,\n\n00:02:36.128 --> 00:02:39.185\nand it would continue on the other legs\nto blow up more mines.\n\n00:02:39.209 --> 00:02:42.995\nAnd the colonel who was in charge\nof this testing exercise\n\n00:02:43.019 --> 00:02:45.137\nends up calling it off,\n\n00:02:45.161 --> 00:02:47.596\nbecause, he says, it's too inhumane\n\n00:02:47.620 --> 00:02:52.136\nto watch this damaged robot\ndrag itself along the minefield.\n\n00:02:54.936 --> 00:02:58.833\nNow, what would cause\na hardened military officer\n\n00:02:58.857 --> 00:03:00.900\nand someone like myself\n\n00:03:00.924 --> 00:03:02.781\nto have this response to robots?\n\n00:03:03.495 --> 00:03:06.805\nWell, of course, we're primed\nby science fiction and pop culture\n\n00:03:06.829 --> 00:03:09.408\nto really want to personify these things,\n\n00:03:09.432 --> 00:03:12.221\nbut it goes a little bit deeper than that.\n\n00:03:12.245 --> 00:03:17.554\nIt turns out that we're biologically\nhardwired to project intent and life\n\n00:03:17.578 --> 00:03:22.344\nonto any movement in our physical space\nthat seems autonomous to us.\n\n00:03:23.172 --> 00:03:26.637\nSo people will treat all sorts\nof robots like they're alive.\n\n00:03:26.661 --> 00:03:29.344\nThese bomb-disposal units get names.\n\n00:03:29.368 --> 00:03:31.050\nThey get medals of honor.\n\n00:03:31.074 --> 00:03:33.399\nThey've had funerals for them\nwith gun salutes.\n\n00:03:34.338 --> 00:03:38.171\nAnd research shows that we do this\neven with very simple household robots,\n\n00:03:38.195 --> 00:03:40.330\nlike the Roomba vacuum cleaner.\n\n00:03:40.354 --> 00:03:41.645\n(Laughter)\n\n00:03:41.669 --> 00:03:44.758\nIt's just a disc that roams\naround your floor to clean it,\n\n00:03:44.782 --> 00:03:47.088\nbut just the fact it's moving\naround on its own\n\n00:03:47.112 --> 00:03:49.279\nwill cause people to name the Roomba\n\n00:03:49.303 --> 00:03:52.485\nand feel bad for the Roomba\nwhen it gets stuck under the couch.\n\n00:03:52.509 --> 00:03:54.374\n(Laughter)\n\n00:03:54.398 --> 00:03:57.738\nAnd we can design robots\nspecifically to evoke this response,\n\n00:03:57.762 --> 00:04:01.223\nusing eyes and faces or movements\n\n00:04:01.247 --> 00:04:04.506\nthat people automatically,\nsubconsciously associate\n\n00:04:04.530 --> 00:04:06.550\nwith states of mind.\n\n00:04:06.574 --> 00:04:09.867\nAnd there's an entire body of research\ncalled human-robot interaction\n\n00:04:09.891 --> 00:04:11.717\nthat really shows how well this works.\n\n00:04:11.741 --> 00:04:14.867\nSo for example, researchers\nat Stanford University found out\n\n00:04:14.891 --> 00:04:16.892\nthat it makes people really uncomfortable\n\n00:04:16.916 --> 00:04:19.388\nwhen you ask them to touch\na robot's private parts.\n\n00:04:19.412 --> 00:04:21.532\n(Laughter)\n\n00:04:21.556 --> 00:04:23.579\nSo from this, but from many other studies,\n\n00:04:23.603 --> 00:04:27.826\nwe know, we know that people\nrespond to the cues given to them\n\n00:04:27.850 --> 00:04:29.872\nby these lifelike machines,\n\n00:04:29.896 --> 00:04:31.913\neven if they know that they're not real.\n\n00:04:33.612 --> 00:04:37.668\nNow, we're headed towards a world\nwhere robots are everywhere.\n\n00:04:37.692 --> 00:04:40.757\nRobotic technology is moving out\nfrom behind factory walls.\n\n00:04:40.781 --> 00:04:43.794\nIt's entering workplaces, households.\n\n00:04:43.818 --> 00:04:50.027\nAnd as these machines that can sense\nand make autonomous decisions and learn\n\n00:04:50.051 --> 00:04:52.603\nenter into these shared spaces,\n\n00:04:52.627 --> 00:04:55.123\nI think that maybe the best\nanalogy we have for this\n\n00:04:55.147 --> 00:04:57.082\nis our relationship with animals.\n\n00:04:57.481 --> 00:05:01.369\nThousands of years ago,\nwe started to domesticate animals,\n\n00:05:01.393 --> 00:05:05.438\nand we trained them for work\nand weaponry and companionship.\n\n00:05:05.462 --> 00:05:10.447\nAnd throughout history, we've treated\nsome animals like tools or like products,\n\n00:05:10.471 --> 00:05:12.645\nand other animals,\nwe've treated with kindness\n\n00:05:12.669 --> 00:05:15.747\nand we've given a place in society\nas our companions.\n\n00:05:15.771 --> 00:05:19.620\nI think it's plausible we might start\nto integrate robots in similar ways.\n\n00:05:21.442 --> 00:05:24.538\nAnd sure, animals are alive.\n\n00:05:24.562 --> 00:05:25.712\nRobots are not.\n\n00:05:27.584 --> 00:05:30.164\nAnd I can tell you,\nfrom working with roboticists,\n\n00:05:30.188 --> 00:05:33.710\nthat we're pretty far away from developing\nrobots that can feel anything.\n\n00:05:35.030 --> 00:05:36.490\nBut we feel for them,\n\n00:05:37.793 --> 00:05:39.000\nand that matters,\n\n00:05:39.024 --> 00:05:42.651\nbecause if we're trying to integrate\nrobots into these shared spaces,\n\n00:05:42.675 --> 00:05:47.303\nwe need to understand that people will\ntreat them differently than other devices,\n\n00:05:47.327 --> 00:05:49.171\nand that in some cases,\n\n00:05:49.195 --> 00:05:52.367\nfor example, the case of a soldier\nwho becomes emotionally attached\n\n00:05:52.391 --> 00:05:54.438\nto the robot that they work with,\n\n00:05:54.462 --> 00:05:56.966\nthat can be anything\nfrom inefficient to dangerous.\n\n00:05:58.509 --> 00:06:00.647\nBut in other cases,\nit can actually be useful\n\n00:06:00.671 --> 00:06:03.294\nto foster this emotional\nconnection to robots.\n\n00:06:04.142 --> 00:06:06.276\nWe're already seeing some great use cases,\n\n00:06:06.300 --> 00:06:08.904\nfor example, robots working\nwith autistic children\n\n00:06:08.928 --> 00:06:12.562\nto engage them in ways\nthat we haven't seen previously,\n\n00:06:12.586 --> 00:06:16.586\nor robots working with teachers to engage\nkids in learning with new results.\n\n00:06:17.391 --> 00:06:18.772\nAnd it's not just for kids.\n\n00:06:19.708 --> 00:06:22.931\nEarly studies show that robots\ncan help doctors and patients\n\n00:06:22.955 --> 00:06:24.382\nin health care settings.\n\n00:06:25.493 --> 00:06:27.303\nThis is the PARO baby seal robot.\n\n00:06:27.327 --> 00:06:30.612\nIt's used in nursing homes\nand with dementia patients.\n\n00:06:30.636 --> 00:06:32.206\nIt's been around for a while.\n\n00:06:32.230 --> 00:06:35.555\nAnd I remember, years ago,\nbeing at a party\n\n00:06:35.579 --> 00:06:38.150\nand telling someone about this robot,\n\n00:06:38.174 --> 00:06:40.300\nand her response was,\n\n00:06:40.324 --> 00:06:41.586\n\"Oh my gosh.\n\n00:06:42.466 --> 00:06:43.654\nThat's horrible.\n\n00:06:45.014 --> 00:06:48.411\nI can't believe we're giving people\nrobots instead of human care.\"\n\n00:06:50.498 --> 00:06:52.373\nAnd this is a really common response,\n\n00:06:52.397 --> 00:06:54.896\nand I think it's absolutely correct,\n\n00:06:54.920 --> 00:06:56.960\nbecause that would be terrible.\n\n00:06:57.753 --> 00:07:00.237\nBut in this case,\nit's not what this robot replaces.\n\n00:07:00.816 --> 00:07:03.936\nWhat this robot replaces is animal therapy\n\n00:07:03.960 --> 00:07:07.158\nin contexts where\nwe can't use real animals\n\n00:07:07.182 --> 00:07:08.350\nbut we can use robots,\n\n00:07:08.374 --> 00:07:13.604\nbecause people will consistently treat\nthem more like an animal than a device.\n\n00:07:15.460 --> 00:07:17.840\nAcknowledging this emotional\nconnection to robots\n\n00:07:17.864 --> 00:07:19.833\ncan also help us anticipate challenges\n\n00:07:19.857 --> 00:07:23.308\nas these devices move into more intimate\nareas of people's lives.\n\n00:07:24.069 --> 00:07:27.473\nFor example, is it OK\nif your child's teddy bear robot\n\n00:07:27.497 --> 00:07:29.734\nrecords private conversations?\n\n00:07:29.758 --> 00:07:33.821\nIs it OK if your sex robot\nhas compelling in-app purchases?\n\n00:07:33.845 --> 00:07:35.241\n(Laughter)\n\n00:07:35.265 --> 00:07:37.766\nBecause robots plus capitalism\n\n00:07:37.790 --> 00:07:41.495\nequals questions around\nconsumer protection and privacy.\n\n00:07:42.507 --> 00:07:44.119\nAnd those aren't the only reasons\n\n00:07:44.143 --> 00:07:46.713\nthat our behavior around\nthese machines could matter.\n\n00:07:48.705 --> 00:07:51.975\nA few years after that first\ninitial experience I had\n\n00:07:51.999 --> 00:07:54.310\nwith this baby dinosaur robot,\n\n00:07:54.334 --> 00:07:56.835\nI did a workshop\nwith my friend Hannes Gassert.\n\n00:07:56.859 --> 00:07:59.756\nAnd we took five\nof these baby dinosaur robots\n\n00:07:59.780 --> 00:08:02.233\nand we gave them to five teams of people.\n\n00:08:02.257 --> 00:08:03.954\nAnd we had them name them\n\n00:08:03.978 --> 00:08:07.787\nand play with them and interact with them\nfor about an hour.\n\n00:08:08.665 --> 00:08:10.871\nAnd then we unveiled\na hammer and a hatchet\n\n00:08:10.895 --> 00:08:13.173\nand we told them to torture\nand kill the robots.\n\n00:08:13.197 --> 00:08:16.204\n(Laughter)\n\n00:08:16.815 --> 00:08:19.109\nAnd this turned out to be\na little more dramatic\n\n00:08:19.133 --> 00:08:20.411\nthan we expected it to be,\n\n00:08:20.435 --> 00:08:23.507\nbecause none of the participants\nwould even so much as strike\n\n00:08:23.531 --> 00:08:24.838\nthese baby dinosaur robots,\n\n00:08:24.862 --> 00:08:30.012\nso we had to improvise a little,\nand at some point, we said,\n\n00:08:30.036 --> 00:08:34.473\n\"OK, you can save your team's robot\nif you destroy another team's robot.\"\n\n00:08:34.497 --> 00:08:36.352\n(Laughter)\n\n00:08:36.797 --> 00:08:38.992\nAnd even that didn't work.\nThey couldn't do it.\n\n00:08:39.016 --> 00:08:40.167\nSo finally, we said,\n\n00:08:40.191 --> 00:08:42.223\n\"We're going to destroy all of the robots\n\n00:08:42.247 --> 00:08:44.532\nunless someone takes\na hatchet to one of them.\"\n\n00:08:45.544 --> 00:08:49.123\nAnd this guy stood up,\nand he took the hatchet,\n\n00:08:49.147 --> 00:08:51.853\nand the whole room winced\nas he brought the hatchet down\n\n00:08:51.877 --> 00:08:53.657\non the robot's neck,\n\n00:08:53.681 --> 00:09:00.019\nand there was this half-joking,\nhalf-serious moment of silence in the room\n\n00:09:00.043 --> 00:09:01.741\nfor this fallen robot.\n\n00:09:01.765 --> 00:09:03.171\n(Laughter)\n\n00:09:03.195 --> 00:09:06.889\nSo that was a really\ninteresting experience.\n\n00:09:06.913 --> 00:09:09.372\nNow, it wasn't a controlled\nstudy, obviously,\n\n00:09:09.396 --> 00:09:12.246\nbut it did lead to some\nlater research that I did at MIT\n\n00:09:12.270 --> 00:09:14.498\nwith Palash Nandy and Cynthia Breazeal,\n\n00:09:14.522 --> 00:09:18.149\nwhere we had people come into the lab\nand smash these HEXBUGs\n\n00:09:18.173 --> 00:09:21.260\nthat move around in a really\nlifelike way, like insects.\n\n00:09:21.284 --> 00:09:24.418\nSo instead of choosing something cute\nthat people are drawn to,\n\n00:09:24.442 --> 00:09:26.535\nwe chose something more basic,\n\n00:09:26.559 --> 00:09:30.039\nand what we found\nwas that high-empathy people\n\n00:09:30.063 --> 00:09:32.206\nwould hesitate more to hit the HEXBUGS.\n\n00:09:33.533 --> 00:09:35.097\nNow this is just a little study,\n\n00:09:35.121 --> 00:09:37.510\nbut it's part of a larger body of research\n\n00:09:37.534 --> 00:09:40.478\nthat is starting to indicate\nthat there may be a connection\n\n00:09:40.502 --> 00:09:42.875\nbetween people's tendencies for empathy\n\n00:09:42.899 --> 00:09:44.875\nand their behavior around robots.\n\n00:09:45.679 --> 00:09:49.306\nBut my question for the coming era\nof human-robot interaction\n\n00:09:49.330 --> 00:09:52.385\nis not: \"Do we empathize with robots?\"\n\n00:09:53.169 --> 00:09:56.089\nIt's: \"Can robots change\npeople's empathy?\"\n\n00:09:57.447 --> 00:09:59.734\nIs there reason to, for example,\n\n00:09:59.758 --> 00:10:02.091\nprevent your child\nfrom kicking a robotic dog,\n\n00:10:03.186 --> 00:10:06.100\nnot just out of respect for property,\n\n00:10:06.124 --> 00:10:09.077\nbut because the child might be\nmore likely to kick a real dog?\n\n00:10:10.465 --> 00:10:12.348\nAnd again, it's not just kids.\n\n00:10:13.522 --> 00:10:17.578\nThis is the violent video games question,\nbut it's on a completely new level\n\n00:10:17.602 --> 00:10:22.362\nbecause of this visceral physicality\nthat we respond more intensely to\n\n00:10:22.386 --> 00:10:23.933\nthan to images on a screen.\n\n00:10:25.632 --> 00:10:28.210\nWhen we behave violently towards robots,\n\n00:10:28.234 --> 00:10:31.354\nspecifically robots\nthat are designed to mimic life,\n\n00:10:31.378 --> 00:10:35.270\nis that a healthy outlet\nfor violent behavior\n\n00:10:35.294 --> 00:10:37.838\nor is that training our cruelty muscles?\n\n00:10:39.469 --> 00:10:40.619\nWe don't know ...\n\n00:10:42.580 --> 00:10:46.525\nBut the answer to this question has\nthe potential to impact human behavior,\n\n00:10:46.549 --> 00:10:49.317\nit has the potential\nto impact social norms,\n\n00:10:49.341 --> 00:10:53.190\nit has the potential to inspire rules\naround what we can and can't do\n\n00:10:53.214 --> 00:10:54.365\nwith certain robots,\n\n00:10:54.389 --> 00:10:56.237\nsimilar to our animal cruelty laws.\n\n00:10:57.186 --> 00:11:00.050\nBecause even if robots can't feel,\n\n00:11:00.074 --> 00:11:03.154\nour behavior towards them\nmight matter for us.\n\n00:11:04.847 --> 00:11:07.702\nAnd regardless of whether\nwe end up changing our rules,\n\n00:11:08.884 --> 00:11:12.440\nrobots might be able to help us\ncome to a new understanding of ourselves.\n\n00:11:14.234 --> 00:11:16.550\nMost of what I've learned\nover the past 10 years\n\n00:11:16.574 --> 00:11:18.812\nhas not been about technology at all.\n\n00:11:18.836 --> 00:11:21.339\nIt's been about human psychology\n\n00:11:21.363 --> 00:11:23.966\nand empathy and how we relate to others.\n\n00:11:25.482 --> 00:11:27.847\nBecause when a child is kind to a Roomba,\n\n00:11:29.220 --> 00:11:33.235\nwhen a soldier tries to save\na robot on the battlefield,\n\n00:11:33.259 --> 00:11:36.897\nor when a group of people refuses\nto harm a robotic baby dinosaur,\n\n00:11:38.206 --> 00:11:41.397\nthose robots aren't just motors\nand gears and algorithms.\n\n00:11:42.459 --> 00:11:44.364\nThey're reflections of our own humanity.\n\n00:11:45.481 --> 00:11:46.632\nThank you.\n\n00:11:46.656 --> 00:11:50.053\n(Applause)\n"
}
